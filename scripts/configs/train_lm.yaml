model:
  vocab_size: 50257
  context_length: 64
  d_model: 256
  num_layers: 2
  num_heads: 2
  d_ff: 256
  rope_theta: 10000

optimizer:
  lr: 
  weight_decay: 0.01
  betas: [0.9, 0.95]
  eps: 1e-8

data:
  data_path: "data/TinyStories_yyy_small_tokens.bin"
  batch_size: 32
  context_length: 64
  device: "cpu"  # or "cuda"

training:
  train_steps: 1000
  eval_interval: 100
  eval_iters: 200
  save_interval: 2000
  checkpoint_path: "checkpoints/lm_checkpoint.pt"
  grad_clip_norm: 1.0
  lr_max: 1e-3
  warmup_steps: 500
  lr_decay_steps: 8000
  lr_min: 1e-5
  cosine_iters: true