#è¿™æ®µä»£ç çš„åæœŸæ¯”è¾ƒæœ‰é—®é¢˜ï¼Œéœ€è¦ä¿®æ”¹

import os
import sys
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

from tokenizer.fast_tokenizer import FastTokenizer
import numpy as np
import time
from tqdm import tqdm

def count_lines(filename):
    """å¿«é€Ÿç»Ÿè®¡æ–‡ä»¶è¡Œæ•°"""
    with open(filename, 'r', encoding='utf-8') as f:
        return sum(1 for _ in f)

# åˆå§‹åŒ– Fast Tokenizer
print("Initializing Fast Tokenizer...")
tokenizer = FastTokenizer(
    vocab_path="data/TinyStories-train_vocab.json",
    merges_path="data/TinyStories-train_merges.txt",
    special_tokens=["<|endoftext|>"]
)

input_file = "data/TinyStoriesV2-GPT4-valid.txt"
output_file = "data/TinyStoriesV2-GPT4-valid.bin"

# ç»Ÿè®¡æ€»è¡Œæ•°ï¼ˆå¯é€‰ï¼Œç”¨äºæ˜¾ç¤ºè¿›åº¦æ¡ï¼‰
print("Counting lines...")
total_lines = count_lines(input_file)
print(f"Total lines: {total_lines:,}")

print("\nStarting tokenization...")
start_time = time.time()

# ä½¿ç”¨ tqdm æ˜¾ç¤ºè¿›åº¦æ¡
with open(output_file, "wb") as out_f:
    with open(input_file, "r", encoding="utf-8") as in_f:
        total_tokens = 0
        
        # ä½¿ç”¨ tqdm åŒ…è£…æ–‡ä»¶è¿­ä»£å™¨
        for line in tqdm(in_f, total=total_lines, desc="Tokenizing", unit="lines"):
            # ç¼–ç æ¯ä¸€è¡Œ
            token_ids = tokenizer.encode(line)
            # å†™å…¥äºŒè¿›åˆ¶æ–‡ä»¶
            np.array(token_ids, dtype=np.uint16).tofile(out_f)
            total_tokens += len(token_ids)

end_time = time.time()
elapsed_time = end_time - start_time

print(f"\n{'='*60}")
print(f"Tokenization completed!")
print(f"Total lines:  {total_lines:,}")
print(f"Total tokens: {total_tokens:,}")
print(f"Time taken:   {elapsed_time:.2f} seconds")
print(f"Speed:        {total_tokens / elapsed_time:,.0f} tokens/sec")
print(f"Output file:  {output_file}")
print(f"File size:    {os.path.getsize(output_file) / (1024**2):.2f} MB")
print(f"{'='*60}")

# éªŒè¯ç¼–ç ç»“æœ
print("\nVerifying the encoded file...")
tokens = np.fromfile(output_file, dtype=np.uint16)
print(f"Loaded {len(tokens):,} tokens from binary file")

if len(tokens) == total_tokens:
    print("âœ“ Token count matches!")
else:
    print(f"âœ— Token count mismatch: expected {total_tokens:,}, got {len(tokens):,}")

# æ˜¾ç¤ºå‰ 200 ä¸ªå­—ç¬¦
print("\nFirst 200 characters of decoded text:")
decoded_sample = tokenizer.decode(tokens[:100].tolist())
print(decoded_sample[:200])
print("\n" + "="*60)

# è¿™AIå†™çš„ä¹Ÿå¤ªæŠ½è±¡äº†ï¼Œä½ é•¿åº¦éƒ½ä¸ä¸€æ ·é‚£è‚¯å®šä¸ç­‰å•Š ğŸ¤£
# # æ¯”è¾ƒä¸åŸæ–‡ä»¶çš„ä¸€è‡´æ€§
# print("\nComparing with original text...")
# with open(input_file, "r", encoding="utf-8") as f:
#     original_text = f.read(1000)  # è¯»å–å‰ 1000 å­—ç¬¦

# # æ‰¾å‡ºå¯¹åº”çš„ token æ•°é‡
# test_tokens = []
# char_count = 0
# with open(input_file, "r", encoding="utf-8") as f:
#     for line in f:
#         line_tokens = tokenizer.encode(line)
#         test_tokens.extend(line_tokens)
#         char_count += len(line)
#         if char_count >= 1000:
#             break

# decoded_text = tokenizer.decode(test_tokens)
# if decoded_text == original_text:
#     print("âœ“ Decoded full text matches original!")
# else:
#     print("âš  Decoded text may differ slightly (due to tokenization boundaries)")
#     print("length of original:", len(original_text))
#     print("length of decoded:", len(decoded_text))
#     print(f"\nFirst 200 chars of original:\n{original_text[:200]}")
#     print(f"\nFirst 200 chars of decoded:\n{decoded_text[:200]}")