# LLM_from_scratch

Build a Large Language Model (LLM) completely from scratch! ðŸ˜„

## âœ… Todo

### Tokenizer
- [x] Implement a Byte Pair Encoding (BPE) tokenizer  
  - [x] Train the tokenizer on a corpus  
  - [x] Save and load the tokenizer  
  - [x] Encode and decode text  
  - [x] Validate the tokenizer  

### Model architecture
- [x] Build a Transformer model  
  - [x] Implement the soft_max
  - [x] Implement the attention mechanism  
  - [x] Implement the feed-forward network  
  - [x] Implement positional encoding  
  - [x] Implement layer normalization  

### Train
- [ ] Train the model on a dataset 
  - [x] Implement the cross_entropy loss function
  - [x] Implement the Adam optimizer
  - [] Set up the training loop
  - [] Monitor training progress

### Test
- [ ] Evaluate the model's performance